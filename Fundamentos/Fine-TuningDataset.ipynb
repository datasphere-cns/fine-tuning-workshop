{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102755f1-f251-44ea-a0b7-b8595ec86cc7",
   "metadata": {},
   "source": [
    "# Reporte de Preparación de Datos para Fine-tuning con OpenAI\n",
    "\n",
    "**Fecha:** 7 de junio de 2025\n",
    "\n",
    "**Nombre:** Nelson Zepeda\n",
    "\n",
    "**Correo Electrónico:** nelson.zepeda@datasphere.tech\n",
    "\n",
    "**Dataset:** https://huggingface.co/datasets/Recognai/sentiment-banking/viewer/default/train\n",
    "---\n",
    "\n",
    "### Descripción del Archivo Generado\n",
    "\n",
    "Este documento detalla el proceso de preparación de un dataset para el workshop de Fine-tuning con OpenAI. Específicamente, se ha tomado un archivo en formato Parquet alojado en Hugging Face (que contiene datos de `inputs` y `prediction` de un modelo de análisis de sentimiento bancario) y se ha transformado a un archivo JSONL (`.jsonl`). Este formato JSONL es el requerido por OpenAI para el fine-tuning de sus modelos de lenguaje, adaptándolos a una tarea de clasificación de texto.\n",
    "\n",
    "---\n",
    "\n",
    "### Marco Teórico\n",
    "\n",
    "#### ¿Qué es Hugging Face?\n",
    "\n",
    "**Hugging Face** es una empresa y una plataforma que se ha convertido en un centro neurálgico para el desarrollo y la implementación de modelos de Machine Learning, especialmente en el campo del Procesamiento de Lenguaje Natural (NLP) y la Visión por Computadora. Sus componentes más destacados incluyen:\n",
    "\n",
    "* **Transformers Library:** Una librería de código abierto que proporciona arquitecturas de modelos de transformadores (como BERT, GPT, T5) pre-entrenados y listos para usar, facilitando tareas como la clasificación de texto, la generación de texto, la traducción y la respuesta a preguntas.\n",
    "* **Hugging Face Hub:** Una plataforma centralizada que aloja miles de modelos, datasets y demos pre-entrenados por la comunidad y empresas. Permite compartir, descubrir y reutilizar recursos de ML de manera sencilla. Los datasets y modelos se pueden acceder directamente a través de URLs o APIs, como se vio en el uso del archivo Parquet.\n",
    "* **Datasets Library:** Una librería que simplifica la descarga y el preprocesamiento de datasets para tareas de ML, optimizando el manejo de grandes volúmenes de datos.\n",
    "\n",
    "Hugging Face promueve la investigación abierta y la democratización del acceso a las herramientas de IA, lo que la convierte en un recurso invaluable para desarrolladores y científicos de datos.\n",
    "\n",
    "#### ¿Qué es Parquet?\n",
    "\n",
    "**Parquet** es un formato de almacenamiento de datos columnar de código abierto, diseñado para un almacenamiento y procesamiento de datos eficiente, especialmente en el contexto de grandes volúmenes de datos (Big Data) y sistemas de procesamiento analítico. Sus características clave incluyen:\n",
    "\n",
    "* **Almacenamiento Columnar:** A diferencia de los formatos de fila (como CSV), Parquet almacena los datos columna por columna. Esto es altamente eficiente para cargas de trabajo analíticas donde a menudo solo se necesita acceder a un subconjunto de columnas, ya que se pueden leer solo las columnas relevantes sin cargar toda la fila en memoria.\n",
    "* **Compresión y Codificación Eficientes:** Ofrece altos ratios de compresión y varias técnicas de codificación que reducen el tamaño del archivo en disco, lo que se traduce en menos I/O (entrada/salida) y un procesamiento más rápido.\n",
    "* **Esquema de Datos (Schema Evolution):** Soporta esquemas complejos y la evolución de esquemas, lo que significa que el esquema de los datos puede cambiar con el tiempo sin romper los datos existentes.\n",
    "* **Soporte Multi-lenguaje:** Compatible con múltiples lenguajes y herramientas, incluyendo Python (Pandas, PyArrow), Java (Spark), R, etc.\n",
    "\n",
    "Por su eficiencia en almacenamiento y velocidad de consulta, Parquet es un formato popular para lagos de datos y sistemas de procesamiento de datos distribuidos.\n",
    "\n",
    "---\n",
    "\n",
    "### Descripción del Script Python para la Extracción y Formateo\n",
    "\n",
    "El script Python desarrollado tiene como objetivo principal transformar un dataset de análisis de sentimiento bancario almacenado en formato Parquet a un archivo JSONL (`.jsonl`) compatible con los requisitos de fine-tuning de OpenAI para modelos de chat.\n",
    "\n",
    "**Funcionalidad del Script:**\n",
    "\n",
    "1.  **Carga del Dataset Parquet:**\n",
    "    * Utiliza la librería `pandas` y `huggingface_hub` (necesaria para el protocolo `hf://`) para leer directamente el archivo Parquet desde su ubicación en el Hugging Face Hub.\n",
    "    * `df = pd.read_parquet(\"hf://datasets/Recognai/sentiment-banking/data/train-00000-of-00001.parquet\")`\n",
    "\n",
    "2.  **Selección y Limpieza de Columnas Relevantes:**\n",
    "    * Se identifican las columnas `inputs` (que contiene el texto original del usuario) y `prediction` (que contiene la etiqueta de sentimiento predicha por un modelo base).\n",
    "    * Se eliminan las filas que tienen valores nulos en cualquiera de estas dos columnas, asegurando que cada entrada de entrenamiento sea completa.\n",
    "\n",
    "3.  **Procesamiento de la Columna `inputs`:**\n",
    "    * La columna `inputs` inicialmente contenía el texto en un formato de objeto (e.g., `{\"text\": \"Mi tarjeta dejó de funcionar...\"}`).\n",
    "    * Se implementa la función `extract_text_from_inputs` para extraer solo la cadena de texto pura de este objeto, ya que OpenAI espera contenido de usuario como una cadena simple. Se manejan casos donde la entrada podría no ser un diccionario o un JSON válido.\n",
    "\n",
    "4.  **Procesamiento de la Columna `prediction`:**\n",
    "    * La columna `prediction` contenía una cadena que representaba una lista de diccionarios con la etiqueta y el score (e.g., `[{'label': 'NEGATIVE', 'score': 0.999...}, {'label': 'POSITIVE', 'score': 0.000...}]`).\n",
    "    * Se implementa la función `get_dominant_label` para:\n",
    "        * Determinar el tipo de dato de la entrada (`str`, `list`, `numpy.ndarray`).\n",
    "        * Si es una cadena, usa `ast.literal_eval` para convertirla de forma segura a una lista de diccionarios.\n",
    "        * Si ya es una lista, la utiliza directamente.\n",
    "        * Identifica la etiqueta (`label`) dentro de la lista de diccionarios que tiene el `score` más alto.\n",
    "        * Maneja robustamente errores de formato o valores inesperados, devolviendo \"UNKNOWN\" si no puede extraer una etiqueta válida. Esto asegura que el `content` del rol `assistant` sea una única etiqueta de clase (`\"POSITIVE\"` o `\"NEGATIVE\"`).\n",
    "\n",
    "5.  **Formato de Mensajes de OpenAI:**\n",
    "    * Se define la función `create_openai_message` que toma las columnas procesadas (`processed_text` y `processed_label`) y las estructura en el formato de mensajes de chat que OpenAI requiere:\n",
    "        ```json\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"El texto del usuario\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"La etiqueta de sentimiento\"}\n",
    "        ]}\n",
    "        ```\n",
    "\n",
    "6.  **Generación y Guardado del Archivo JSONL:**\n",
    "    * La función `create_openai_message` se aplica a cada fila del DataFrame procesado para construir una lista de diccionarios en el formato de OpenAI.\n",
    "    * Finalmente, esta lista se escribe en un archivo JSONL (`banking_sentiment_finetune_final.jsonl`), donde cada objeto JSON ocupa una línea separada.\n",
    "\n",
    "Este script es un paso crucial para transformar datos tabulares o semi-estructurados en un formato utilizable para el fine-tuning de modelos de lenguaje, permitiendo que el modelo aprenda a clasificar textos bancarios basándose en los ejemplos proporcionados."
   ]
  },
  {
   "cell_type": "raw",
   "id": "99d7ad62-c356-4af1-a201-3324cb0cb3e2",
   "metadata": {},
   "source": [
    "#pip install pyarrow\n",
    "#pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6c2456-8aca-4577-adb9-bb6495178ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'banking_sentiment_finetune_final.jsonl' generado exitosamente\n",
      "Se procesaron 5001 ejemplos.\n",
      "\n",
      "Las primeras 3 entradas del archivo JSONL son:\n",
      "{\"messages\": [{\"role\": \"user\", \"content\": \"My card stopped working after multiple transactions. Why?\"}, {\"role\": \"assistant\", \"content\": \"NEGATIVE\"}]}\n",
      "{\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I made a transfer from France two days ago and thought it would be here by now. Can you give me an update please?\"}, {\"role\": \"assistant\", \"content\": \"POSITIVE\"}]}\n",
      "{\"messages\": [{\"role\": \"user\", \"content\": \"Has my top-up been cancelled?\"}, {\"role\": \"assistant\", \"content\": \"NEGATIVE\"}]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast \n",
    "import numpy as np \n",
    "\n",
    "# Ruta al archivo parquet en Hugging Face\n",
    "parquet_file_path = \"hf://datasets/Recognai/sentiment-banking/data/train-00000-of-00001.parquet\"\n",
    "\n",
    "try:\n",
    "    # Cargar el dataset Parquet\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "    # Nombres de las columnas que nos interesan\n",
    "    text_column_name = 'inputs'\n",
    "    label_column_name = 'prediction'\n",
    "\n",
    "    # Asegurarse de que las columnas existen y no tienen valores nulos críticos\n",
    "    if text_column_name not in df.columns or label_column_name not in df.columns:\n",
    "        raise ValueError(f\"Las columnas '{text_column_name}' o '{label_column_name}' no se encontraron en el DataFrame.\")\n",
    "\n",
    "    # Filtrar filas donde 'inputs' o 'prediction' puedan ser nulos\n",
    "    df_cleaned = df.dropna(subset=[text_column_name, label_column_name]).copy() # Usar .copy() para evitar SettingWithCopyWarning\n",
    "\n",
    "\n",
    "    # Procesar la columna 'inputs' para extraer solo el texto\n",
    "    def extract_text_from_inputs(input_obj):\n",
    "        try:\n",
    "            # Si ya es un diccionario, accede a 'text'\n",
    "            if isinstance(input_obj, dict) and 'text' in input_obj:\n",
    "                return input_obj['text']\n",
    "            # Si es una cadena que parece JSON, parsea y luego accede\n",
    "            elif isinstance(input_obj, str):\n",
    "                parsed = json.loads(input_obj)\n",
    "                if isinstance(parsed, dict) and 'text' in parsed:\n",
    "                    return parsed['text']\n",
    "            # Si no es un dict ni una cadena parseable con 'text', devuelve el original como string\n",
    "            return str(input_obj)\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            return str(input_obj) # Fallback si no se puede parsear o no tiene 'text'\n",
    "    \n",
    "    df_cleaned['processed_text'] = df_cleaned[text_column_name].apply(extract_text_from_inputs)\n",
    "\n",
    "\n",
    "    # --- CORRECCIÓN FINAL AQUÍ: Procesar la columna 'prediction' para extraer solo la etiqueta ---\n",
    "    def get_dominant_label(prediction_data):\n",
    "        # Caso 1: prediction_data ya es una lista de diccionarios (Pandas ya lo parseó)\n",
    "        if isinstance(prediction_data, list):\n",
    "            pred_list = prediction_data\n",
    "        # Caso 2: prediction_data es una cadena que necesita ser parseada\n",
    "        elif isinstance(prediction_data, str):\n",
    "            try:\n",
    "                # Eliminar saltos de línea y espacios extra para ast.literal_eval\n",
    "                cleaned_str = prediction_data.replace('\\n ', '').strip()\n",
    "                pred_list = ast.literal_eval(cleaned_str)\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Si la cadena no es una lista de diccionarios válida, manejar el error\n",
    "                # print(f\"Advertencia: No se pudo parsear la cadena: '{prediction_data}'\") # Descomentar para depurar\n",
    "                return \"UNKNOWN\"\n",
    "        # Caso 3: prediction_data es un numpy array u otro tipo inesperado\n",
    "        elif isinstance(prediction_data, np.ndarray):\n",
    "            # Si es un numpy array, intentamos convertirlo a lista y luego procesar\n",
    "            try:\n",
    "                pred_list = prediction_data.tolist()\n",
    "            except AttributeError: # Si tolist() no está disponible\n",
    "                # print(f\"Advertencia: Tipo de datos inesperado para array: {type(prediction_data)}\") # Descomentar para depurar\n",
    "                return \"UNKNOWN\"\n",
    "        else:\n",
    "            # print(f\"Advertencia: Tipo de datos inesperado para prediction: {type(prediction_data)}\") # Descomentar para depurar\n",
    "            return \"UNKNOWN\" # Retorna \"UNKNOWN\" para tipos de datos no manejados\n",
    "\n",
    "        # Ahora que tenemos pred_list (o un fallo ya ocurrió)\n",
    "        if pred_list and isinstance(pred_list, list):\n",
    "            try:\n",
    "                # Asegurarse de que cada elemento en pred_list es un diccionario con 'label' y 'score'\n",
    "                if all(isinstance(x, dict) and 'label' in x and 'score' in x for x in pred_list):\n",
    "                    # Encuentra el diccionario con el score más alto\n",
    "                    dominant_pred = max(pred_list, key=lambda x: x['score'])\n",
    "                    return dominant_pred['label']\n",
    "            except (KeyError, TypeError):\n",
    "                # print(f\"Advertencia: Formato inesperado en elementos de la lista: {pred_list}\") # Descomentar para depurar\n",
    "                return \"UNKNOWN\"\n",
    "        return \"UNKNOWN\" # Si pred_list está vacía o no es una lista válida\n",
    "\n",
    "    df_cleaned['processed_label'] = df_cleaned[label_column_name].apply(get_dominant_label)\n",
    "\n",
    "\n",
    "    # Función para crear el formato de mensaje de OpenAI con las columnas procesadas\n",
    "    def create_openai_message(row):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row['processed_text']},\n",
    "            {\"role\": \"assistant\", \"content\": row['processed_label']}\n",
    "        ]\n",
    "        return {\"messages\": messages}\n",
    "\n",
    "    # Aplicar la función a cada fila para crear la lista de entradas JSONL\n",
    "    openai_data = df_cleaned.apply(create_openai_message, axis=1).tolist()\n",
    "\n",
    "    # Definir el nombre del archivo JSONL de salida\n",
    "    output_jsonl_file = 'banking_sentiment_finetune_final.jsonl' # Otro nombre para la versión más robusta\n",
    "\n",
    "    # Guardar a JSONL\n",
    "    with open(output_jsonl_file, 'w', encoding='utf-8') as f:\n",
    "        for entry in openai_data:\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "    print(f\"Archivo '{output_jsonl_file}' generado exitosamente\")\n",
    "    print(f\"Se procesaron {len(openai_data)} ejemplos.\")\n",
    "    print(\"\\nLas primeras 3 entradas del archivo JSONL son:\")\n",
    "    with open(output_jsonl_file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se pudo encontrar el archivo parquet en la ruta: {parquet_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error general al procesar el archivo: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
